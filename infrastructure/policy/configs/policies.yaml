---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kyverno-policies
  namespace: kyverno
spec:
  interval: 10m0s
  driftDetection:
    mode: enabled
  chart:
    spec:
      chart: kyverno-policies
      version: 3.6.x
      sourceRef:
        kind: HelmRepository
        name: kyverno
        namespace: kyverno
      interval: 10m0s
  values:
    validationFailureAction: Audit

    policyExclude:
      disallow-host-path:
        any:
        - resources:
            kinds:
            - DaemonSet
            - Pod
            namespaces:
            - infra-observability
            - infra-security
            names:
            - '*node-exporter*'
            - 'vector*'
            - 'node-collector*'
            - 'falco-*'
      disallow-host-namespaces:
        any:
        - resources:
            kinds:
            - DaemonSet
            - Pod
            namespaces:
            - infra-observability
            - infra-security
            names:
            - '*node-exporter*'
            - 'node-collector*'
      disallow-privileged-containers:
        any:
        - resources:
            kinds:
            - DaemonSet
            - Pod
            namespaces:
            - infra-security
            names:
            - 'falco-*'
---
apiVersion: kyverno.io/v2
kind: ClusterCleanupPolicy
metadata:
  name: cleanup-finished-pods
  annotations:
    policies.kyverno.io/title: Remove Finished Pods
    policies.kyverno.io/category: Cluster Maintenance
    policies.kyverno.io/subject: Pod
    policies.kyverno.io/severity: info
    policies.kyverno.io/description: >-
      This policy removes 'dead' pods which occur after e.g. a talos node reboot.
      Normally they would be removed with:

      kubectl delete pod --field-selector=status.phase==Suceeded -A
      kubectl delete pod --field-selector=status.phase==Failed -A
spec:
  match:
    any:
    - resources:
        kinds:
        - Pod
  conditions:
    all:
    - key: '{{ target.status.phase }}'
      operator: AnyIn
      value: [Succeeded, Failed]
  schedule: '0 * * * *'
---
apiVersion: kyverno.io/v2
kind: ClusterCleanupPolicy
metadata:
  name: cleanup-failed-jobs
  annotations:
    policies.kyverno.io/title: Delete Failed Jobs
    policies.kyverno.io/category: Cluster Maintenance
    policies.kyverno.io/subject: Job
    policies.kyverno.io/severity: info
    policies.kyverno.io/description: >-
      This policies removes failed jobs as soon as there has been a more recent
      successfull job execution.
      Normalliy they would be deleted with:

      kubectl delete job --field-selector=status.phase==Failed -A
spec:
  match:
    any:
    - resources:
        kinds:
        - Job
        selector:
          matchLabels:
            k8s.mcathome.ch/cleanup: 'true'
  conditions:
    all:
    - key: "{{ target.status.failed || '0' }}"
      operator: GreaterThan
      value: 0
  schedule: '0 1 * * *'